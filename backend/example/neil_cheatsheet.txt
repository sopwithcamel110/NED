Asymptotic Bounds Formal Definition
o(g(n)) = { f(n) |âˆ€c > 0, âˆƒğ‘›0 > 0, âˆ€n â‰¥ ğ‘›0 : 0 â‰¤ f(n) < cg(n) }
ğ‘‚(ğ‘”(ğ‘›)) = { ğ‘“(ğ‘›) | âˆƒğ‘ > 0, âˆƒğ‘›0 > 0, âˆ€ğ‘› â‰¥ ğ‘›0: 0 â‰¤ ğ‘“(ğ‘›) â‰¤ ğ‘ğ‘”(ğ‘›) }
Î˜(ğ‘”(ğ‘›)) = { ğ‘“(ğ‘›) | âˆƒğ‘1,ğ‘2 > 0, âˆƒğ‘›0 > 0, âˆ€ğ‘› â‰¥ ğ‘›0: 0 â‰¤ ğ‘1ğ‘”(ğ‘›) â‰¤ ğ‘“(ğ‘›) â‰¤ ğ‘2ğ‘”(ğ‘›)
Î©(ğ‘”(ğ‘›)) = { ğ‘“(ğ‘›) | âˆƒğ‘ > 0, âˆƒğ‘›0 > 0, âˆ€ğ‘› â‰¥ ğ‘›0 âˆ¶ 0 â‰¤ ğ‘ğ‘”(ğ‘›) â‰¤ ğ‘“(ğ‘›)
ğœ”(g(n)) = { f(n) |âˆ€c > 0, âˆƒğ‘›0 > 0, âˆ€n â‰¥ ğ‘›0 : 0 â‰¤ cg(n) < f(n) }

Simplified master theorem
T(n) = a * T(n/b) + ğœƒ(nd)
a>bd -> T(n)= ğœƒ(n ^ logb(a))
a=bd -> T(n)= ğœƒ(nd log(n))
a<bd -> T(n)= ğœƒ(nd)

Logs
log_a(x*y) = log_a(x) + log_a(y)
log_a(x/y) = log_a(x) - log_a(y)
log_a(x^y) = y * log_a(x)
log_a(a^r) = r
log_a(a) = 1
log_a(1) = 0
log_a(1/b) = - log_a(b)
log_b(x) = Î¸(log_a(x)) log_b(x) 
              = Î¸(log_a(x))
(a^(b^n)) â‰  (a^b)^n
(x^y)^z = x^(y*z)
a^(log_a(b)) = b
a^(log_b(n)) = n^(log_b(a))
log_b(a) = log_x(a)/log_x(b)
log_b(n)=x -> b^x = n

Union find
Union-Find data structure, also called a Disjoint-Set data structure Used for storing collections of sets Supports:
â€¢ makeSet(u): Create a set {u} O(n)
â€¢ find(u): Return the set that u is in O(1)
â€¢ union(u,v): Merge the set that u is in with the set that
Let R be a binary relation on a set S. Then R is an Equivalence Relation on S iff:
â€¢ R is Reflexive on S: R(x,x)
â€¢ R is Symmetric on S: If R(x,y) then R(y,x)
â€¢ R is Transitive on S: If R(x,y) and R(y,z) then R(x,z)
Example: Integers mod m  Two integers are equivalent if they have the same remainder mod m. IF R is an Equivalence Relation on S, you can partition S into equivalence Classes induced by R, where all elements x and y in each Equivalence Class satisfy R(x,y).
Let S = { a, b, c, d, e}
Let R = { (a,a), (b,b), (c,c), (d,d), (e,e) (a,b), (b,a), (a,c), (c,a), (b,c), (c,b), (d,e), (e,d) }
This R can be partitioned into two subsets, such that within each subset, all the elements are related (to themselves and each other) in both directions.
P = { {a, b, c}, {d, e}} These two subsets of S are the Equivalence Classes induced by R.
[For integers mod m, there are m Equivalence Classes, corresponding to 0, 1, 2, â€¦ m-1.]

Find:
Letâ€™s assume that we have an Equivalence Relation R, and a set of induced Equivalence Classes: {E1, E2 , E3 , ... , Ek}
â€¢ Each Ei will have a representative element.
â€¢ FIND(x) for x in S, returns the representative element for the Equivalence Class containing x
FIND -- an Example
Let S = { a, b, c, d, e}
P = { {a, b, c}, {d, e}}
E1 = {a, b, c}, E2 = {d, e}
Let a and d be the representative elements of E1 ,E2, respectively.
FIND(c) -> a
FIND(d) -> d
FIND(e) -> d

Strong Induction instructions:
Base Step: Prove that the proposition P(n0) is true.
Induction Step: Prove for all n > n0 that if P(k) is true for all k such that n0 â‰¤ k < n, then P(n) must also be true.
To do this, take an arbitrary n > n0, and assume that P(k) is true for all k such that n0 â‰¤ k < n then show that P(n) must also be true.
When the Base Step and the Induction Step have been proved, we conclude that P(n) must be true for all n â‰¥ n0

Variation of strong induction
Base Step: Prove that the proposition P(n0) is true. Induction Step: Prove for all n >= n0 that if P(k) is true for all k such that n0 â‰¤ k â‰¤ n, then P(n+1) must also be true.
To do this, take an arbitrary n >= n0, and assume that P(k) is true for all k such that n0 â‰¤ k â‰¤ n then show that P(n+1) must also be true.
When the Base Step and the Induction Step have been proved, we conclude that P(n) must be true for all n â‰¥ n0

Weak Induction instructions
Base Step: Prove that the proposition P(n0) is true.
Induction Step: Prove for all n > n0 that if P(n-1) is true, then P(n) must also be true.
To do this, take an arbitrary n > n0, and assume that P(n-1) is true for that n then show that P(n) must also be true.
When the Base Step and the Induction Step have been proved, we conclude that P(n) must be true for all n â‰¥ n0

L'Hopital's Rule | Calc reminders
lim nâ†’âˆ f(n)/g(n) = lim nâ†’âˆ fâ€™(n) / gâ€™(n) 
y=loga(x) if ay=x  logb(x) = ğœƒ(loga(x))

True Statements About Asymptotic Limits
If lim nâ†’âˆ f(n) / g(n) 
= 0, then f(n) = o(g(n)). (converse true)
= L, where 0 â‰¤ L < âˆ, then f(n) = O(g(n))
= L, where 0 < L < âˆ, then f(n) = ğœƒ(g(n))
= L, where 0 < L â‰¤ âˆ, then f(n) = Î©(g(n))
= âˆ, then f(n) = ğœ”(g(n)). (converse true)
f(n) is asymptotically equivalent to g(n):
f(n) ~ g(n) only if lim nâ†’âˆf(n)/g(n) =1

Masters theorem
T(n) = a T(n/b) + f(n), Constant ğœ€ > 0
f(n) = O(n ^ (logba - ğœ€)) -> T(n)= ğœƒ(n ^ logba).
f(n) = ğœƒ(n ^ logba) -> T(n)= ğœƒ( (n ^ logba) * lg n)
f(n) = Î©(n ^ (logba + ğœ€)) if a f(n/b) â‰¤ c f(n)
for c < 1 -> T(n)= ğœƒ(f(n))

Loop Invariant: At the start
Initialization: base case of loop
Maintenance: function of loop 
Termination: state at end

Harmonic Numbers
Hn = 1/1 + Â½ + â…“ + â€¦ 1/n
Hn  ~ ln(n) 
(Asymptotically Equivalent)

Stirlingâ€™s Approximation:
n! = sqrt(2Ï€n) * (n/e)n 
Stirlingâ€™s Formula:
n!= sqrt(2Ï€n) * (n/e)n  * (1 +  Î˜(1/n))
Corollaries:
n! ~ sqrt(2Ï€n) * (n/e)n
1) n! = o(nn) 
2) n! = ğœ”(bn) for any b>0
3) log(n!)= ğœƒ(n log(n))

Insert MaxHeap
// Increase heapsize by 1, putting in new value â€œkeyâ€, and
// maintaining the heap property for the resulting array A.
Heaplnsert(A, heapsize, key)
  heapSize = heapSize + 1
  A[heapSize] = key
  j = heapSize
  // Propagate change further up tree if parent(j) value is smaller than A[j].//
  while j > 1 and A[ parent(j) ] < A[j]/.
    Swap A[j] with A[ parent(j) ]
    j = parent(j)

Recursion Tree Table: 
for a * T(n/b) + dn,
Problem | size | work
Level t and bottom
at | n/bt | d*(at*n/bt)
alg(n) | n/blg(n) | ğœƒ(n^lg(a)

BinaryTree as Array
root i = 1
parent(i)= i/2
left(i) = 2i
right(i)= 2i+1

PARTITION(A, p, r)
//Quicksort and select
x = A[p]
q = p
for j = p + 1 to r
  if A[j] â‰¤ x
    q = q + 1
    exchange A[q] and A [j]
exchange A[p] and A[q]
return q

QuickSort(A, p, r):
If p < r
  q = PARTITION(A, p, r)
  // Values in A[p..q] are less than or equal to A[q]
  // Values in A[q+1..r] are greater than A[q]
  QuickSort(A, p, q-1)
QuickSort(A, q+1 ,r)
To sort entire array A: QuickSort(A, 1, A.length)

Connected Components Example
V = {a, b, c, d, e}
E = { (a, b), (a,c), (d,e) }
Let V = {a, b, c, d, e} and E = { (a, b), (a,c), (d,e) }
â€¢ Start with an initial partition into singleton sets.
â€¢ P = { {a}, {b}, {c}, {d}, {e} }
â€¢ Process one edge at a time. (a, b)
â€¢ Perform FIND operations on each element, then UNION on the results: UNION(FIND(a), FIND(b))
â€¢ Continue until all the edges have been processed

HeapSort Sorting Strategy: O(n log n)
1. Run BuildMaxHeap(A,n) on an unordered array A[1..n], and Set heapSize = n.
2. Find max element A[1].
3. Swap elements A[heapSize] and A[1]. Now the max element is at the end of the array!
4. Discard node heapSize from heap (by decrementing heapSize variable).
5. New root may violate MaxHeap Property, but its children must be MaxHeaps! So run MaxHeapify(A, heapSize, 1) to fix this.
6. Go back to Step 2 if heapSize is not 0.

MaxHeapify(A, heapSize, parent) pseudocode
left_child = left(parent)
right_child = right(parent)
largest = parent
if (left_child <= heapSize and A[left_child] > A[largest])
largest = left_child
if (right_child <= heapSize and A[right_child] > A[largest])
largest = right_child
if largest â‰  parent
Swap A[parent] and A[largest]
MaxHeapify(A, heapSize, largest)
SELECT(A, k, p, r)
If r-p+1 <= n0 // MergeSort for â€œsmallâ€ n
  MergeSort(A, p, r)
  return p + k -1
pivotPstn = CHOOSEPIVOT(A, p, r)
q = PARTITION(A, pivotPstn, p, r)
Lcount = q - p
if Lcount = k-1 // kâ€™th smallest value is in position q
  Return q
else if Lcount > k-1 // kâ€™th smallest value is on the Left
  return SELECT(A, k, p, q-1)
else // Lcount < k-1, so kâ€™th smallest value is on the Right
  return SELECT(A, k-(Lcount+1), q+1, r)

Lower Bound of Î©(n log(n))
Any deterministic Comparison-Based Sorting Algorithm must take Î©(n log(n)) steps (Worst-Case Running Time)
Any randomized Comparison-Based Sorting Algorithm must take Î©(n log(n)) steps in Expectation (Average Running Time).

Decision Tree Theorems 
Theorem 1:  Let T be a binary tree with n leaves and height h. Then necessarily â„ â‰¥ âŒˆlg ğ‘›âŒ‰.
Theorem 2: Suppose a problem P has ğ‘“(ğ‘›) possible outputs on input of size n. Then no algorithm for P that uses only k-ary probes of the input data can perform fewer than âŒˆlogğ‘˜ ğ‘“(ğ‘›)âŒ‰ such probes on input of size n, in worst case. Thus âŒˆlogğ‘˜ ğ‘“(ğ‘›)âŒ‰ is a lower bound for the (worst case) runtime of such an algorithm. IThis does not mean an algorithm that solves P in only âŒˆlogğ‘˜ ğ‘“(ğ‘›)âŒ‰ steps exists, only that none do in fewer. Also note that the theorem is restricted to the class of algorithms doing only k-ary probes. Changing k changes the lower bound. Since all log functions are asymptotically equivalent, the theorem implies a single asymptotic lower bound Î©(log ğ‘“(ğ‘›)) for any k.
Theorem 3: Any algorithm that performs only array comparisons must do at least âŒˆ3ğ‘›/2âŒ‰ âˆ’ 2 such comparisons in order to determine both the minimum and maximum of an array of length n

slowPrim( G = (V,E), starting vertex s ):
//Let (s,u) be the lightest edge coming out of s.
  MST = { (s,u) }
  verticesVisited = { s, u }
  while |verticesVisited| < |V|:
    Find the lightest edge (x,v) in E such that:
     x is in verticesVisited, and
     v is not in verticesVisited
    Add (x,v) to MST
    Add v to verticesVisited
  return MST

Fast Prims:
Until all vertices are reached:
  Activate the unreached vertex u with the smallest key.
  For each of uâ€™s neighbors v:
    k[v] = min( k[v], weight(u,v) )
    if k[v] updated, ğœ‹[v] = u
  Mark u as reached, and add (ğœ‹[u],u) to MS

The UNION Operation
â€¢ Letâ€™s assume that we have an Equivalence Relation R, and a set of induced Equivalence Classes:  {E1, E2 , E3 , ... , Ek}
â€¢ UNION(x, y) changes the partition so that the sets that x and y belong to are merged into one set. For example, if x âˆˆ E2 and y âˆˆ E3, then after UNION(x, y) we have: {E1, E2 U E3 , ... , Ek }  {E1, E2 U E3 , ... , Ek}

Application of UNION-FIND
Let G = (V, E) be an Undirected Graph. We want to find the Connected Components of G.
â€¢ Two vertices u and v are in the same Connected Component if there is a path between them.
This defines an Equivalence Relation on the Graph. Which means they are Reflexive, Symmetric, Transitive
â€¢ Two vertices are equivalent if they are in the same Connected Component.

Graph Handout:
Lemma 1: If T is a tree with n vertices and m edges, then m = n -1
Lemma 2: If G is an acyclic graph with n vertices, m edges, and k connected components, then m = n - k
Lemma 3: If G is a connected graph with n vertices and m edges, then m >= n - 1
Lemma 4: Lemma 4 If G is a graph with n vertices, m edges, and k connected components, then m >= n - k
Lemma 5: Let G be a connected graph with n vertices and m edges. Suppose also that m = n -1. Then G is acyclic, and hence a tree.
Lemma 6: Let G be an acyclic graph with n vertices and m edges. Suppose also that m = n -1. Then G is connected, and hence a tree
Lemma 7: Let G be a connected graph with n vertices and m edges. Suppose also that m = n. Then G contains exactly one cycle. (Such a graph is called unicyclic.

Theorem 1: (The Treeness Theorem) Let G = (V,E) be a graph. The following are equivalent.
a) G is a tree (i.e. connected and acyclic).
b) G contains a unique x-y path for any x, y in V
c) G is connected, but if any edge is removed from E, the resulting graph is disconnected.
d) G is connected, and |E| = |V| -1
e) G is acyclic, and |E| = |V| -1
f) G is acyclic, but if any edge is added to E (joining two non-adjacent vertices), then the resulting
graph contains a unique cycle

Greedy Algorithm(Activity selection):
  Suppose the activities are sorted by finishing time
  If not, sort them. Okay, now theyâ€™re sorted.
  myActivityList = [] 
  for k = 1,â€¦,n:
    if we can fit in Activity k after the last thing in myActivityList:
        myActivityList.Append(Activity k)
  return myActivityList
  O(n) If list is already sorted, O(n log n) else

Up-Trees An Up-Tree is a tree in which each node (except the root) has a single â€œpointerâ€ that points to its Parent. A set of Up-Trees is called a Forest of Up-Trees. A Forest of Up-Trees can represent the subsets for the UNION-FIND ADT

FIND Using Up-Trees: To determine the representative element for x, start at the node for x and follow the links to the root of xâ€™s Up-Tree.

UNION Using Up-Trees:
To merge two subsets, make the root of one Up-Tree point to the root of the other. UNION(a, f)

Worst-Case Time Using â€œBasicâ€ Up-Trees
â€¢ FIND(x): O(path length from x to root of xâ€™s tree) = O(n)
â€¢ UNION(x,y): O(1)

DynamicBinomialCoefficient(ğ‘›, ğ‘˜)
ğ¶[0] = 1
for ğ‘– = 1 to ğ‘˜
  ğ¶[ğ‘–] = 0
  for ğ‘— = 1 to ğ‘›
    for ğ‘– = ğ‘˜ down to 1
      ğ¶[ğ‘–] = ğ¶[ğ‘– âˆ’ 1] + ğ¶[ğ‘–]
return ğ¶[ğ‘˜]

Enhanced FIND
FIND(x): While performing this operation, compress the paths of nodes encountered along the way, so that they are closer to the root.
a. Full Path Compression. After finding the root r, make another pass from x to r making each node along the way point to r. 
b. Path Halving: Make every node along the path from x to r point to its grandparent.
c. Path Splitting: Make every other node along the path from x to r point to its grandparent.
Path Halving and Path Splitting are more efficient than Full Path Compression, but they have same Worst-Case complexity. Unlike Full path compression, they donâ€™t require a second pass.

Enhanced UNION
UNION(x,y): Try to make the smaller tree point to the root of the larger.
Rank method: Try to use the higher root as the new root.
â€¢ Difficult to update height easily when path compression is done with FIND; we can use â€œrankâ€ instead, which is an estimate of height.
â€¢ When x and y have different ranks, make the tree with the smaller rank point to the root of the tree with the larger rank, and keep ranks as they were.
â€¢ When x and y have the same rank, make one of them point to the other, and add 1 to the rank of the one thatâ€™s now the overall root.
Size method: Try to use the root of the larger Up-Tree as the new root.
â€¢ Maintain a node count at the root of each Up-Tree.
â€¢ When a new node gets inserted, it will have to FIND the root, so itâ€™s easy to maintain this count.
â€¢ Count at nodes that are not the root doesnâ€™t matter, so it doesnâ€™t have to be maintained.

Unbounded knapsack 
UnboundedKnapsack(W, n, weights, values):
  K[0] = 0
  for x = 1, â€¦, W:
    K[x] = 0
    for i = 1, â€¦, n:
      if wi â‰¤ x:
        K[x] = max { K[x], K[x â€“ wi] + vi }
  return K[W]

Unbounded knapsack that also finds the items 
UnboundedKnapsack(W, n, weights, values):
  K[0] = 0
  ITEMS[0] = âˆ…
  for x = 1, â€¦, W:
    K[x] = 0
    for i = 1, â€¦, n:
      if wi â‰¤ x:
        K[x] = max { K[x], K[x â€“ wi] + vi }
        If K[x] was updated:
          ITEMS[x] = ITEMS[x â€“ wi] âˆª { item i }
  return K[W]

The Substitution Method (Handout)
We begin with the following example.
ğ‘‡(ğ‘›) = { 2 1 â‰¤ ğ‘› < 3
3ğ‘‡(âŒŠğ‘›/3âŒ‹) + ğ‘› ğ‘› â‰¥ 3
Suppose that we are able to guess somehow that ğ‘‡(ğ‘›) = ğ‘‚(ğ‘› log(ğ‘›)). In order to prove this guess, we must find positive numbers c and ğ‘›0 such that ğ‘‡(ğ‘›) â‰¤ ğ‘ğ‘› log(ğ‘›) for all ğ‘› â‰¥ ğ‘›0. If we knew appropriate values for these constants, we could prove this inequality by induction. Our goal then, is to determine c and ğ‘›0 such that an induction proof can be made to work.
Observe that the recurrence itself contains two base cases: ğ‘› = 1 and ğ‘› = 2. This indicates that the induction proof may also require multiple base cases. However, the inequality to be proved ğ‘‡(ğ‘›) â‰¤ ğ‘ğ‘› log(ğ‘›), is actually false in the case ğ‘› = 1, since ğ‘‡(1) = 2 and log(1) = 0. Since log(2) â‰  0, the same problem does not occur at ğ‘› = 2. Indeed, for ğ‘› = 2 we seek to show ğ‘‡(2) â‰¤ ğ‘ â‹… 2 â‹… log(2), which can be made true by a proper choice of ğ‘. Therefore we take the lowest base case to be ğ‘›0 = 2. It remains to determine the highest base case, which we will denote by ğ‘›1. We begin by mimicking the induction step IId, with lowest base case ğ‘› = 2 and highest base case ğ‘› = ğ‘›1. In what follows, it will be algebraically
convenient to take log() to mean log3(). Let ğ‘› > ğ‘›1, and assume ğ‘‡(ğ‘˜) â‰¤ ğ‘ğ‘˜ log( ğ‘˜) for all k in the range 2 â‰¤ ğ‘˜ < ğ‘›. In particular, when ğ‘˜ = âŒŠğ‘›/3âŒ‹ we have ğ‘‡(âŒŠğ‘›/3âŒ‹) â‰¤ ğ‘âŒŠğ‘›/3âŒ‹ log(âŒŠğ‘›/3âŒ‹). We must show that ğ‘‡(ğ‘›) â‰¤ ğ‘ğ‘› log( ğ‘›). We have
ğ‘‡(ğ‘›) = 3ğ‘‡(âŒŠğ‘›/3âŒ‹) + ğ‘›      (by the recurrence for T)
â‰¤ 3ğ‘âŒŠğ‘›/3âŒ‹ log(âŒŠğ‘›/3âŒ‹) + ğ‘› (by the induction hypothesis)
â‰¤ 3ğ‘(ğ‘›/3) log( ğ‘›/3) + ğ‘›    (since âŒŠğ‘¥âŒ‹ â‰¤ ğ‘¥ for all x)
= ğ‘ğ‘›(log( ğ‘›) âˆ’ 1) + ğ‘›
= ğ‘ğ‘› log( ğ‘›) âˆ’ ğ‘ğ‘› + ğ‘›
To obtain ğ‘‡(ğ‘›) â‰¤ ğ‘ğ‘› log( ğ‘›), we need to have âˆ’ğ‘ğ‘› + ğ‘› â‰¤ 0, which will be true if ğ‘ â‰¥ 1. Thus as long as c is chosen to satisfy the constraint ğ‘ â‰¥ 1, the induction step will go through. It remains to determine the constant ğ‘›1, which represents the highest base case. Since we only use the induction hypothesis in the case ğ‘˜ = âŒŠğ‘›/3âŒ‹, we require that 2 â‰¤ âŒŠğ‘›/3âŒ‹ < ğ‘› whenever ğ‘› > ğ‘›1 . This is equivalent to ğ‘› > ğ‘›1 â‡’ 6 â‰¤ ğ‘›, which indicates we should choose ğ‘›1  = 5. The base cases are then ğ‘› = 2, 3, 4, 5, and it is required that
ğ‘‡(2) â‰¤ ğ‘ â‹… 2 â‹… log(2),
ğ‘‡(3) â‰¤ ğ‘ â‹… 3 â‹… log(3),
ğ‘‡(4) â‰¤ ğ‘ â‹… 4 â‹… log(4),
ğ‘‡(5) â‰¤ ğ‘ â‹… 5 â‹… log(5)
One checks that ğ‘‡(2) = 2, ğ‘‡(3) = 9, ğ‘‡(4) = 10 and ğ‘‡(5) = 11. To satisfy all constraints then, we take ğ‘ to be the maximum of the numbers { 1, 2/2 log(2) , 9/3 log(3) , 10/4 log(4) , 11/5 log(5) }. A few comparisons reveal this maximum to be ğ‘ = 9/3log3(3) = 3. It is important to realize that we have not proved anything yet. Everything we have done up to this point has been scratch work with the goal of finding appropriate values for ğ‘ and ğ‘›0. It remains to present a complete induction proof of the assertion: ğ‘‡(ğ‘›) â‰¤ 3ğ‘›log(ğ‘›) for all ğ‘› â‰¥ 2.

Kruskal(G = (V,E)):
  Sort E by weight in non-decreasing order
  MST = {}  // initialize an empty tree
  for v in V:
    makeSet(v) // put each vertex in its own tree in the forest
  for (u,v) in E: // go through the edges in sorted order
    if find(u) != find(v): // if u and v are not in the same tree. Handles matter!
      add (u,v) to MST
      union(u,v) // merge uâ€™s tree with vâ€™s tree
  return MST

Ackermann Function A(i,j):
j + 1 if i = 0
A(i âˆ’ 1,1) if i > 0 and j = 0
A(i âˆ’ 1, A i, j âˆ’ 1) if i > 0 and j > 0
A(i,j) grows very rapidly. For example, A(4,3) = 2^65336 - 3 So the function f(k) = A(k,k) also grows very rapidly. Its inverse function, written Î±(n) is called the Inverse Ackermann Function, and it grows very slowly
Î±(n) = min { k | A(k,k) â‰¥ n }
Î±(n) < lgk(n) = min { k | lg ( lg ( lg â€¦ (n) ) ) â‰¤ 1 }
[where lg is applied k times]

Proof by Induction:Huffman Encoding
Induction Hypothesis:After the itâ€™th step: There is an optimal tree containing the current subtrees as â€œleavesâ€
Base Case: After the 0â€™th step:There is an optimal tree containing all the characters.
Induction Step: Suppose that the Induction Hypothesis holds for t-1 ,After t-1 steps, there is an optimal tree containing all the current sub-trees as â€œleaves.â€
Want to show: After t steps, there is an optimal tree containing all the current sub-trees as leaves.
Lemma 1: If x and y are the two least-frequent letters, there is an optimal subtree where x and y are siblings.
Lemma 2: Suppose that there is an optimal tree containing as a subtree. Then we might as well replace that subtree with a new letter that has frequency
Conclusion :After the last step:There is an optimal tree containing this whole tree as a subtree.That is:After the last step, the tree that weâ€™ve constructed is optimal.
Greedy Scheduling Solution
scheduleJobs( JOBS ):Sort jobs by the ratio: ri=ci/ti = cost of delaying job i/ time job i takes to complete
For each i, let sorted_JOBS[i] be the job with the iâ€™th biggest ri
Return sorted_JOBS.The running time is O(nlog(n))
Greedy Scheduling Induction
Inductive hypothesis:There is an optimal ordering so that the first t jobs are sorted_JOBS[1..t].
Base case:When t=0, this says that: â€œThere is an optimal ordering so that the first 0 jobs are [].â€ Thatâ€™s true.
Inductive Step: This says that: There is an optimal ordering on sorted_JOBS[t+1..n] in which sorted_JOBS[t+1] is first. That follows from our Lemma.(Given jobs where Job i takes time ti with cost ci ,there is an optimal schedule in which the first job is the one that maximizes ratio ci/ti)
Conclusion: When t=n, this says that: â€œThere is an optimal ordering in which the first n jobs are sorted_JOBS.â€ So what we returned is an optimal ordering of all n jobs

CoinChange(ğ‘‘, ğ‘)
 ğ¶[1 â‹¯ ğ‘›; 0 â‹¯ ğ‘]
ğ‘› = length[ğ‘‘]
for ğ‘– = 1 to ğ‘›
  ğ¶[ğ‘–, 0] = 0
for ğ‘– = 1 to ğ‘›
  for ğ‘— = 1 to ğ‘
    if ğ‘– = 1 and ğ‘— < ğ‘‘[1]
      ğ¶[1, ğ‘—] = âˆ
    else if ğ‘– = 1
      ğ¶[1, ğ‘—] = 1 + ğ¶[1, ğ‘— âˆ’ ğ‘‘[1]]
    else if ğ‘— < ğ‘‘[ğ‘–]
     ğ¶[ğ‘–, ğ‘—] = ğ¶[ğ‘– âˆ’ 1, ğ‘—]
    else
      ğ¶[ğ‘–, ğ‘—] = min(ğ¶[ğ‘– âˆ’ 1, ğ‘—], 1 + ğ¶[ğ‘–, ğ‘— âˆ’ ğ‘‘[ğ‘–]])
return ğ¶[ğ‘›, ğ‘]

Some Lemmas for Disjoint-Set Operations with Union by Rank and Path Compression
Amortized cost is taken across m MAKE-SET, UNION and FIND operations, which are converted into O(m) MAKE-SET, LINK and FIND operations. UNION uses 2 FIND operations and 1 LINK operation.
Lemma: The cost of each MAKE-SET operation is O(1).
Lemma: The amortized cost of each LINK operation is O(Î±(n)).
Lemma: The amortized cost of each FIND operation is O(Î±(n)).
Theorem: Any sequence of m MAKE-SET, UNION and FIND operations, n of which are MAKE-SET operations, can be performed on a Disjoint Set Forest in worst-case time O(m Î±(n)).
â€¢ The UNION operations are done by Rank.
â€¢ FIND operations use Path Compression (any version works).
In theory, this is not linear, but in practice it is, â€¦ since log*n â‰¤  4 for values of n that come up in practice.

MAKE-SET(x)
  ğœ‹[x] = x
  rank[x] = 0

UNION(x,y)
  LINK(FIND(x), FIND(y))

LINK(x, y)
  if rank[x] > rank[y]
    ğœ‹[y] = x
  else ğœ‹[x] = y
    if rank[x] = rank[y]
      rank[y] = rank[y] + 1

FIND(x)
  if x â‰  ğœ‹[x]
    ğœ‹[x] = FIND(ğœ‹[x])
  return ğœ‹[x]

Dijkstraâ€™s Algorithm
  Mark all nodes as unsure.
  Set d[s] = 0, and set d[v] = âˆ for all other nodes.
    Loop
      Pick the not-sure node u with the smallest estimate d[u].
         Update all uâ€™s not-sure neighbors v:
       d[v] â† min( d[v] , d[u] + weight(u,v))
       Mark u as sure.
    Repeat loop until all nodes are marked as sure.

Bellman Ford: O(nm) 
for n vertices and m edges
For v in V:
  d[v] = âˆ
d[s] = 0
For i = 1,..,n-1:
  For each edge e = (u,v) in E:
    d[v] â† min( d[v] , d[u] + weight(u,v) )
For each edge e = (u,v) in E:
  if d[v] > d[u] + weight(u,v)):
    return Negative Cycle Found

Red-Black Trees:
Only info required for final is that all operations including Search, Insert, Delete, Find-Min, Update are O(log n)

Induction example
Prove that for n â‰¥ 0, the nth Fibonacci number F(n) is less than 2n.
Proposition/Statement:
For n â‰¥ 0, the nth Fibonacci number F(n) is less than 2n, where:
F(0) = 0 F(1) = 1
For n â‰¥ 2, F(n) = F(n-1) + F(n-2)
Strong Induction.
Base Cases are for n=0 and n=1. 
F(0) = 0 < 20 =1 F(1) = 1 < 21 =2 Check!
Induction Hypothesis: For all 0 â‰¤ k < n, the kth Fibonacci number F(k) < 2k.
[Strong Induction]
Induction Step: We need to prove that for all n â‰¥ 2, if F(k) < 2k for all k such that 0 â‰¤ k < n, then F(n) < 2n.
Okay, assume that we have a value of n â‰¥ 2, and that for n â‰¥ 2, F(k) < 2k for all k such that 0 â‰¤ k < n. Letâ€™s see if we can prove that F(n) < 2n.
Since n â‰¥ 2, F(n) = F(n-1) + F(n-2). Also since n â‰¥ 2, both n-1 and n-2 are â‰¥ 0, so we can use the Induction Hypothesis, which says that F(n-1) < 2n-1 and F(n-2) < 2n-2. Hence F(n) = F(n-1) + F(n-2) < 2n-1 + 2n-2 < 2n-1 + 2n-1 = 2n.

Insertion-Sort: Correctness Proof Using Loop Invariant with (Weak) Induction
Loop Invariant: At the start of the iteration of the outer loop for value j, the first j-1 elements of the array are sorted, and the original values are still in the array.
Initialization: At the start of the first iteration, variable j is 2. The first element of the array, A[1], is sorted (trivially), and the original values are still in the array
Maintenance: When we begin iteration j, the previous iteration j-1 put A[j-1] where it belongs among the earlier(already sorted) elements of A. So the first j-1 elements of A are sorted, and the original values are still in the array.
Termination: When the variable j equals A.length+1, the loop terminates. The first n = A.length items of A are sorted, and the original values are still in the array, so Insertion-Sort is correct.

Hw6 3b) 
We can reach any vertex if we do DFS(A,0). But if we executed DFS(C, 0), then vertices A and B would not be visited. But we could keep track of any unvisited vertices. Then, after executing DFS(C, 0), if any vertices were unvisited, we could find an unvisited vertex (e.g., B) and execute DFS(B, x+1), where x is the value returned by DFS(C, 0). And then we could execute DFS again, as long as there are any unvisited vertices. Whatâ€™s the Asymptotic Running Time of an efficient implementation of this algorithm? Your answer should be expressed using n, the number of vertices and m, the number of edges. Donâ€™t assume that the number of edges is O(n2). Background on this problem: This relates to an algorithm called Topological Sort, which works on acyclic directed graphs. In Topological Sort, you're given a set of edges, and you need to find a sequencing of all the vertices which "respects" the ordering specified by the edges. That means that for each edge, the source vertex appears earlier i the sequence than the corresponding destination vertex. For example, vertices could correspond to tasks, and some tasks might have to be finished before other tasks. If you can only do one task at a time, what's a sequence in which you can accomplish all the tasks? Topological Sort finds such a Sequence

Answer 3b):
An efficient implementation (which is called Topological Sort) would take time O(n) to iterate through all the vertices (to ensure that they were all visited) + O(m) to traverse each of the edges. Why O(m)? No vertex v is visited more than once. Also, each edge (u, v) is traversed only once, when we visit vertex u (which is not visited more than once). So there are two things that are done per vertex, visiting it and iterating through all the vertices to find vertices that havenâ€™t been visited. Hence work done by all vertices is O(n). And the work done per edge is also constant, so the work for all edges is O(m). Hence this algorithm is O(n+m). Thatâ€™s the correct answer. If you assume that m â‰¥ n (reasonable assumption in most situations), then itâ€™s O(m). Explanation for answer was not requested.
(Additional info about Topological Sort follows, for those who are interested in
why this algorithm matters.) Suppose that we think of each edge as saying that a task must be completed
before a subsequent task can be started. So for our graph, that says that A can be started immediately, but (for example) D canâ€™t be started until both A and B have been completed, because there are edges AD and BD. Weâ€™d like to find a legal order in which we can do the tasks, where â€œlegalâ€ means that we honor the requirements given by the edges. Topological Sort gives us a legal order for the tasks if we read off the Finish Times backwards (which is easy to do if we push tasks with new Finish Times on a Stackâ€”sorting is not required).For our example, based on Finish Times:
â€¢ If we went to B before D, we see that A B D C E would be a legal order for the tasks.
â€¢ If we went to D before B, we also see that A B D C E would be a legal order for the tasks. (Same order; thatâ€™s not always the case.)
Thatâ€™s easy to see in this example, but it may be much more difficult to determine a legal order when there are more tasks and more edges. Topological Sort determines a legal order only when the graph has no cycles (but can be written to detect cycles when cycles exist). And the Runtime for Topological Sort is O(n+m), or if we assume that m â‰¥ n, O(m), and thatâ€™s a great Runtime

Strong induction: Merge-Sort returns a sorted array that has the original values in it.
Initial/Base Case (number of elements is 1): A 1-element array is always sorted.
Maintenance: Suppose that:
â€“ the Left half L, which is A[p:q], is the original A[p:q] sorted, and
â€“ the Right half R, which is A[q+1:r], is the original A[q+1:r] sorted.
Then Merge(L,R) places values in A[p:r] that are the original values in A[p:r] sorted.
Termination: The top recursive call, Merge-Sort(A,1,n) places values in A[1:n] that are the original values in A[1:n] sorted.
Hence Merge-Sort correctly sorts the array A[1:n]

Old Final 11c: 
Since the Bellman-Ford algorithm solves Single Source Shortest Path for graphs in which the edge weights are arbitrary values, why would anyone ever choose to use Dijkstraâ€™s algorithm, rather than always using Bellman-Ford?
Answer 11c): Bellman-Ford algorithm runs in time O(nm) on a graph G with n vertices and m edges, which is O(n3) on a graph that has O(n2) edges. Dijkstra is a lot faster. Hope that you justified that using at least one of the following:
â€¢ If we use Red-Black trees, Dijkstra runs in time O((n + m)log(n)), which is O(m log n) if we assume that m â‰¥ n.
â€¢ If we use Priority Queue/Heap the running time is O( n log(n) + m log(n) ), which is O(m log(n)) if we assume that m â‰¥ n.
â€¢ If we use Fibonacci heaps (not discussed in Lecture, so youâ€™re not expected to remember this one), Dijkstra runs in (amortized) runtime O(m + nlog(n))

Induction proof ex: For all integers n â‰¥ 1, the sum of the first n integers is n * (n+1) / 2.
Proposition P(n): For all integers n â‰¥ 1, the sum of the first n integers is n * (n+1) / 2
Base Case P(1): The sum of the first 1 integer is 1, which equals 1 * (1+1)/2. Check!
Induction Hypothesis: For integers n â‰¥ 1, the sum of the first n integers is n * (n+1) / 2.
Induction Step: We need to prove P(n+1),
For integers n â‰¥1, the sum of the first n+1 integers is (n+1) * (n+2) / 2
i(n+1) =  i +f( n+1) = n * (n+1) / 2 + (n+1)

Homework 2 1b) (asymptopic equivalence)
Prove that if f(n) ~ g(n) then f(n) = g(n) + o(g(n))
Assume that f(n) ~ g(n). By definition of ~, that means that ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆ ğ‘“(ğ‘›)/ ğ‘”(ğ‘›) = 1. We need to prove that there is a function h(n) = o(g(n)) such that f(n) = g(n) + h(n). Let h(n) = f(n) - g(n). Then g(n) + h(n) = g(n) + ( f(n) - g(n) ) = f(n), so yes, f(n) = g(n) + h(n). But does h(n) = o(g(n))? To prove that, weâ€™ll show that ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆ â„(ğ‘›)/ ğ‘”(ğ‘›) = 0. ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆ â„(ğ‘›)/ ğ‘”(ğ‘›). = ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆğ‘“(ğ‘›)âˆ’ğ‘”(ğ‘›)) /ğ‘”(ğ‘›) = ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆ[ ğ‘“(ğ‘›) /ğ‘”(ğ‘›) âˆ’ ğ‘”(ğ‘›) /ğ‘”(ğ‘›) ] = ğ‘™ğ‘–ğ‘š ğ‘›â†’âˆğ‘“(ğ‘›)/ ğ‘”(ğ‘›) âˆ’ 1 = 1 â€“ 1 = 0 So for h(n) = f(n) - g(n) we've shown that: f(n) = g(n) + h(n) and h(n) = o(g(n)) are both true,

Homework 4 4b) Old Final 5 5b) jug matching
There are n red and  n blue jugs. One of each red and blue jug match in size. Find the matching sets using only >, =,  < comparisons in Î©(ğ‘› log ğ‘›).
Answer 4b): Let the red jugs be R1, R2 ,..., Rn and the blue jugs be B1, B2 ,..., Bn. Begin by comparing R1 to each of the n blue jugs until a match is found. The blue jugs could be matched to the red jugs in any order. Thus, thereâ€™s a total of ğ‘›! ways to match the n red jugs to the blue jugs, so this problem has ğ‘›! possible answers, and hence its Decision Tree has at least n! leaves.
We proved a Theorem that if a k-ary Decision Tree has n leaves, then the height of the tree must be at leastâ¾logknâ‹. For this problem, k=3, since two jugs could hold equal, amounts or the blue jug could hold more, or the red jug could hold more. Any algorithm can therefore be represented by a decision tree whose height satisfies: h â‰¥ âŒˆlog3(ğ‘›!)âŒ‰ = Î©(log(ğ‘›!)) = Î©(ğ‘› log ğ‘›) using Stirling's Approximation.
[Remember that the base of the log only changes value be a constant factor.] Therefore, any algorithm solving this problem must perform at least Î©(ğ‘› log ğ‘›) comparisons

Hw6 2c:BFS solves SSSP for both directed on undirected graphs. DFS wouldnâ€™t solve the SSSP problem for either directed or undirected graphs. Prove that DFS doesnâ€™t solve SSSP for undirected graphs. Do this by providing a counterexample graph, showing that DFS solves SSSP incorrectly for that counterexample graph. 
Answer 2c): Using DFS, you report the distance to a vertex as soon as you encounter it, based on the depth-first path that youâ€™re on. That doesnâ€™t have to be the shortest path to the vertex (and often, it isnâ€™t). Simple counterexample graph: Suppose that your undirected graph is the Complete Graph on n vertices, with an edge between every pair of vertices, with each edge weight equal to 1. Then the distance that you would report using DFS would be 0 for the source vertex, 1 for the first vertex you reach, 2 for the second vertex, â€¦, and k for the kâ€™th vertex you reach. But the distance for every vertex should be 1.

Hw6 #4: For both an actual MST (if justified as mst) would also be acceptable, but more work
A) Prove that there is a MST for the above graph that includes edge BC
Primâ€™s algorithm finds the Minimum Spanning Tree (MST) no matter which vertex we use as a starting vertex. If we start at vertex C (not vertex B), the first edge that we choose will be BC, because thatâ€™s the lowest cost edge that includes C. We know that this Greedy choice wonâ€™t prevent us from reaching an Optimal Solution, an MST. So there must be an MST that includes edge BC. 
B) Prove that there is a MST for the above graph that includes edges AD, DE, and FG
Kruskalâ€™s algorithm finds the Minimum Spanning Tree (MST), always adding the edge with minimum cost that doesnâ€™t create a cycle. The three edges DE (weight 2), AD (weight 3),and FG (weight 3) are the edges that have the lowest weights in the graph, and they don't cause a cycle. We know that these Greedy choices wonâ€™t prevent us from reaching an Optimal Solution, an MST. So there must be an MST that includes edges AD, DE and FG

Old Midterm 2 Question 4: Using the Substitution Method, prove that the solution of the Recurrence:
T(n) = T(âŒŠn/2âŒ‹) + n2 if n > 1
T(1) = 1
is O(n2 lg(n)) for n â‰¥ 3 by using the guess f(n) = n2 lg(n)). You must use the Substitution Method in your proof. You just have to show the Backward Induction, but you should identify all the Base Cases that will have to be addressed in the Forward Induction.
Answer 4: Proposition: T(n) â‰¤ c*n2 lg(n)) for n â‰¥ n0 for some c and n0 not determined yet.
Letâ€™s go through the Backwards Induction using the guess f(n) = n2 lg(n)).
Induction Hypothesis: T(n) â‰¤ c*n2 lg(n)) for n â‰¥ n0 for some c and n0 to be determined.
Base Cases: That Induction Hypothesis is false when n=1 because lg(1)=0. When n =2, T(2) = 5 and 22 lg(2) = 4, so weâ€™ll need c â‰¥ 5/4 for the Ind Hyp to be true when n=2. Or we could start with n0 = 3 with c=1, since T(3)=10 â‰¤ 81 lg(3). Weâ€™ll do that. The Q2 problem statement included the odd phrase â€œfor n â‰¥ 3â€ as a suggestion that you use n0 = 3.
Backwards Induction Step:
Now letâ€™s assume that T(k) â‰¤ c*k2 lg(k) for 3 â‰¤ k < n. We want to be able to prove that T(n) â‰¤ c*n2 lg(n)).
T(n) = T(âŒŠn/2âŒ‹) + n2
â‰¤ c*( âŒŠn/2âŒ‹ )2 * lg(âŒŠn/2âŒ‹) + n2  By Ind Hyp, since âŒŠn/2âŒ‹ < n
â‰¤ c*( n/2)2 * lg(n/2) + n2       Since âŒŠxâŒ‹ < x and lg is increasing
â‰¤ c*n2/4 * lg(n) + n2                        Multiplying, and since lg(n/2) < lg(n)
= n2 * [c* lg(n)/4 + 1)             Factoring out n2
â‰¤ c*n lg(n) whenever c*lg(n)/4 + 1 â‰¤ c*lg(n)
Okay, when is c*lg(n)/4 + 1 â‰¤ c*lg(n)? By algebra, thatâ€™s true when 1 â‰¤ (3c/4) * lg(n), which is true when 4 â‰¤ 3c lg(n), which is true whenever 2^4 â‰¤ 2^(3c lg(n)), which is true when 16 â‰¤ n3c, which is true (picking c=1) whenever 3 â‰¤ n. So weâ€™ve completed the Backwards Induction, and supposedly shown that T(n) â‰¤ n2 lg(n)) for 3 â‰¤ n. [Or alternatively, weâ€™ve shown that T(n) â‰¤ 5/4 * n2 lg(n)) for 2 â‰¤ n. Weâ€™ll continue using n0= 3, not 2 below. The next discussion on Base Cases similar, but a little different, if we use c=5/4 and 2 â‰¤ n. Either alternative is okay.] Except that our Induction Step wonâ€™t work unless n/2 â‰¥ 3, i.e., unless nâ‰¥ 6, since the Induction Hypothesis only holds when kâ‰¥3.. So weâ€™ll have to check that our guess works not only for 3, but also for 4 and 5. (You only had to identify the Base Cases that require evaluation; you didnâ€™t have to evaluate them. But weâ€™ll evaluate them.)
Weâ€™ve already checked above that T(n) â‰¤ n2 lg(n)) when n=3.
When n=4, T(4) = T(2) + 42 = 5 + 16 = 21 â‰¤ 16 * lg(4) = 32.
When n=5, T(5) = T(2) + 52 = 5 + 25 = 30 â‰¤ 25 * lg(5) since lg(5) > 2. You werenâ€™t required to do the Forward Induction for Q4

Old Midterm 2 Question 7
We used the following algorithm to solve the Longest Common Subsequence (LCS) Problem for strings X and Y, using Dynamic  Programming. The length(X) is m, and the length(Y) is n. X[i] is the ith character of X, and Y[j] is the jth character of Y.
LCS(X, Y):
  C[i,0] = C[0,j] = 0 for all i = 1,â€¦,m, j=1,â€¦n.
  For i = 1,â€¦,m and j = 1,â€¦,n:
    If X[i] = Y[j]:
      C[i,j] = C[i-1,j-1] + 1
    Else:
      C[i,j] = max{ C[i,j-1], C[i-1,j] }
After running this algorithm, the LCS of X and Y has length C[m,n]. But we havenâ€™t found an actual LCS of strings X and Y.

Old Final Question 7 (Longest Weakly Decreasing Subsequence, Dynamic Programming): 
This question involves an efficient to solve the Longest Weakly Decreasing Subsequence problem. We need to find the length of the longest subsequence in an array A[1:n] such that the array values for that subsequence are never getting bigger.
For example, if the values in array A[1:8] are 21, 11, 21, 5, 16, 16, 14, 2:
â€¢ Positions 1, 2, 4 and 8 of A are a Weakly Decreasing Subsequence, with values 21, 11, 5, 2. That subsequence has length 4.
â€¢ But the Longest Weakly Decreasing Subsequence of A has length 6, corresponding to positions 1, 3, 5, 6, 7, 8, with values 21, 21, 16, 16, 4, 2.
â€¢ And the value that we are seeking, the length of the Longest Weakly Decreasing Subsequence, is 6.
7a): What is the Optimal Substructure for a Dynamic Programming solution to this problem?
The Optimal Substructure is that the optimal solution to the Longest Weakly Decreasing Subsequence problem involves computing the length of the Longest Weakly Decreasing Subsequence of the subarray A[1:i] that ends with index i. To calculate L[i], we consider the Longest Weakly Decreasing Subsequence lengths L[j] of all the previous values, and select the greatest such that A[j] â‰¥ A[i] and j<i. If you find any value, you set L[i] to be the maximum (of the appropriate L[j] values) + 1, and if you don't find any, you set L[i] to be 1
7b)Provide the Recurrence that can used to solve this problem, and explain clearly why that Recurrence is correct.
Let L[i] be the length of the Longest Weakly Decreasing Subsequence of an array ending at index i. If there is at least one j such that A[j] â‰¥ A[i] and j < i, then L[i] should equal the max of their L[j] values + 1. But if there aren't any such j, then L[j] should be 1.
Thus, L[i] can be written as:
L[i] = 1 + max(L[j]) //if there is at least one j such that j < i and A[j] â‰¥ A[i]
L[i] = 1 //if there are no such j
Note that the second clause implies that L[1] = 1.
The recurrence is correct because you get an weakly decreasing subsequence if you put A[i] after a sequence that ends in A[j] if and only if j is before it and A[j] â‰¥ A[i]. And the longest of those (+1 since you're appending A[i]) gives you the longest subsequence ending in A[i]. It is, however, possible that A[i] is bigger than all the preceding values, in which case L[i] must be 1
7c) Provide pseudocode for your algorithm.
LongestWeaklyDecreasingSubsequence(A[1:n])
FOR i = 1 TO n DO
  L[i] = 1
  FOR j = 1 TO i-1 DO
    IF ( A[j] â‰¥ A[i] ) THEN // Maybe we can improve L[i]
      L[i] = MAX(L[i], L[j] + 1)
RETURN MAX( L[j] ) //where the maximum is taken over all j from 1 to n
// We could also just keep track of the biggest value of L[i] thatâ€™s ever set in the algorithm so that we donâ€™t have to iterate through the values of L[i] at the end.

Old Midterm 2 Question 7a
Write pseudocode that finds an LCS of X and Y. Itâ€™s okay to find the LCS backwards, appending to a sequence of characters.
You should assume that you already have the C[m,n] array.Your algorithm's runtime should be efficient.
Result = Empty String // Empty Common Subsequence
i = m // Position in X
j = n // Position in Y
WHILE i > 0 AND j > 0 // Looking for matches until a position hits 0
  IF X[i] = Y[j] // Found match; diagonal move
    Append (or Prepend) C[i,j] to Result
    i = i -1
    j = j â€“ 1
  ELSE IF C[i,j] = C[i,j-1] // No match; move left
    j = j â€“ 1
  ELSE IF C[i,j] = C[i-1,j] // No match; move up
    i = i -1
  ELSE SIGNAL ERROR // Impossible
â€¢ Could do the comparisons between C[i,j] and the left/up positions in either order.
â€¢ Must have gotten value of C[i,j] from either left or up neighbor (or both) if X[i] is not equal to Y[j].
â€¢ Okay if you didnâ€™t check for the impossible error case

Old Final #9 Let ğ›¿(s,u) be the distance between vertex s and vertex u. Prove that throughout execution of Dijkstraâ€™s Algorithm, ğ›¿(s,u) â‰¤ d[u] for all nodes u
 9: From the Shortest Path Lecture:
Claim 1: d[v] â‰¥ ğ›¿(s,v) for all v.
Statement: After t iterations of Dijkstra, d[v] â‰¥ ğ›¿(s,v) for all v.
Base Case: At step 0, ğ›¿(s, s) = 0, and Î´(ğ‘ , ğ‘£) â‰¤ âˆ Check!
Induction Hypothesis: The Statement hold at Step t.
Induction Step: Assume the Induction Hypothesis. We want to prove that the Statement also holds at Step t+1. At step t+1, what do we do? We pick the not-sure vertex u with the smallest estimate d[u]. For each not-sure neighbor v of u, we set d[v] â† min( d[v] , d[u] + w(u,v) ) So the only changes that we may make are to the d[v] values for not-sure neighbors of u.
Does the new value of d[v] still satisfy d[v] â‰¥ ğ›¿(s,v)? d[v] is set to be the minimum of the d[v] value at step t and d[u] + w(u,v).
a) d[v] at step t must satisfy d[v] â‰¥ ğ›¿(s,v) based on the Induction Hypothesis.
b) And ğ›¿(ğ‘ , ğ‘£) â‰¤ Î´(ğ‘ , ğ‘¢) + Î´(ğ‘¢, ğ‘£) â‰¤ ğ‘‘[ğ‘¢] + ğ‘¤(ğ‘¢, ğ‘£). Why?
â€¢ Î´(ğ‘ , ğ‘£) â‰¤ Î´(ğ‘ , ğ‘¢) + Î´(ğ‘¢, ğ‘£) because shortest distance from s to v has to be less than or equal to shortest paths from s to v that go through u.
â€¢ ğ›¿(s,v) â‰¤ d[u] based on the Induction Hypothesis.
â€¢ Î´(ğ‘¢, ğ‘£) â‰¤ w(u,v) because thereâ€™s an edge between u and v, so the shortest path between u and v canâ€™t be longer than that edgeâ€™s weight.
â€¢ Hence Î´(ğ‘ , ğ‘¢) + Î´(ğ‘¢, ğ‘£) â‰¤ ğ‘‘[ğ‘¢] + ğ‘¤(ğ‘¢, ğ‘£)
So whichever term of the minimum is taken, the new value of d[v] satisfies d[v] â‰¥ ğ›¿(s,v).
